{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, AutoPeftModel, AutoPeftModelForCausalLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, BitsAndBytesConfig\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from transformers import LlamaForCausalLM\n",
    "from prompt.model.modeling_llama_custom import LlamaForCausalLM as CustomLlamaForCausalLM\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from prompt.utils import *\n",
    "from prompt.model.model import PromptDecoder, AutoPromptDecoder, PromptConfig\n",
    "from prompt.model.kv_cache import *\n",
    "\n",
    "from pprint import pprint\n",
    "from prompt.inference.dynamic_sparse_trees_3_vicuna_7b import *\n",
    " \n",
    "candidate_lists = dynamic_sparse_trees_60\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def timed(wall_times, key):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    wall_times[key].append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPromptDecoder.from_pretrained(\n",
    "    \"hmarkc/ppd-vicuna-7b-v1.3\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.to(device)\n",
    "print(type(model))\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_times = {'init': [], 'candidates': [], 'forward_pass': [], 'evaluation': [], 'update': []}\n",
    "prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Hi, could you share a tale about a big brain that is good at multi-tasking and likes prompt engineering? ASSISTANT:\"\n",
    "input_ids = model.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "with torch.inference_mode():\n",
    "  if not hasattr(model, \"inference_buffers\"):\n",
    "    print(\"Generating buffers\")\n",
    "    model.generate_dynamic_buffers(candidate_lists)\n",
    "  (past_key_values,\n",
    "      past_key_values_data,\n",
    "      current_length_data,\n",
    "  ) = initialize_past_key_values(model.base_model)\n",
    "  with timed(wall_times, 'init'):\n",
    "    logits, prompt_logits = model.start_inference(input_ids, past_key_values, current_length_data)\n",
    "new_token = 0\n",
    "accept_lengths = []\n",
    "print(model.tokenizer.batch_decode(logits.argmax(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.0\n",
    "posterior_threshold = 0.09\n",
    "posterior_alpha = 0.3\n",
    "sampling = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_cache_lengths = []\n",
    "latencies = []\n",
    "for _ in range(512):\n",
    "  with torch.inference_mode():\n",
    "    with timed(wall_times, 'candidates'):\n",
    "      candidates, tree_candidates_embeds = model.generate_candidates(\n",
    "        logits, \n",
    "        prompt_logits, \n",
    "        temperature, \n",
    "        posterior_threshold, \n",
    "        posterior_alpha, \n",
    "        sampling)\n",
    "    kv_cache_lengths.append(past_key_values[0][0].shape[2])\n",
    "    with timed(wall_times, 'forward_pass'):\n",
    "      logits, all_logits = model.tree_decoding(tree_candidates_embeds, past_key_values, input_ids)\n",
    "    latencies.append(wall_times['forward_pass'][-1])\n",
    "    with timed(wall_times, 'evaluation'):\n",
    "      best_candidate, accept_length = model.evaluate_posterior(\n",
    "        logits, \n",
    "        candidates, \n",
    "        temperature, \n",
    "        posterior_threshold, \n",
    "        posterior_alpha,\n",
    "        sampling)\n",
    "    accept_lengths.append(accept_length.cpu().item()+1)\n",
    "    with timed(wall_times, 'update'):\n",
    "            input_ids, logits, prompt_logits, new_token = model.update_inference_inputs(\n",
    "                    input_ids,\n",
    "                    candidates,\n",
    "                    best_candidate,\n",
    "                    accept_length,\n",
    "                    logits,\n",
    "                    all_logits,\n",
    "                    new_token,\n",
    "                    past_key_values_data,\n",
    "                    current_length_data,\n",
    "            )\n",
    "    torch.cuda.empty_cache()\n",
    "    if model.tokenizer.eos_token_id in input_ids[0, :].tolist():\n",
    "      break\n",
    "print(model.tokenizer.decode(input_ids[0], spaces_between_special_tokens=False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_time = sum(wall_times['candidates'])\n",
    "forward_time = sum(wall_times['forward_pass'])\n",
    "evaluation_time = sum(wall_times['evaluation'])\n",
    "update_time = sum(wall_times['update'])\n",
    "total_time = candidate_time + forward_time + evaluation_time + update_time\n",
    "\n",
    "print('Average 1 forward pass time', {k: sum(v)/len(v) for k, v in wall_times.items()}['forward_pass'], 's')\n",
    "print('Token generation speed', new_token.cpu().item()/total_time, 'tokens/s')\n",
    "print('Average accept length', sum(accept_lengths)/len(accept_lengths))\n",
    "\n",
    "# print percentage of time spent on each step\n",
    "print('Percentage of time spent on candidates generation', candidate_time/total_time, 'average time', candidate_time/len(latencies))\n",
    "print('Percentage of time spent on forward pass', forward_time/total_time, 'average time', forward_time/len(latencies))\n",
    "print('Percentage of time spent on evaluation', evaluation_time/total_time, 'average time', evaluation_time/len(latencies))\n",
    "print('Percentage of time spent on update', update_time/total_time, 'average time', update_time/len(latencies))\n",
    "\n",
    "# plot latencies against runs\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(latencies)\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
