[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "prompt-decoding"
version = "1.0"
description = "Cheap and efficient LLM inference Acceleration using Prompting"
packages = [
    { include = "prompt", from = "." }
]
authors = ["Mark (Hao) Chen <hc1620@ic.ac.uk>"]  
readme = "README.md"
license = "Apache-2.0"
keywords = ["LLM", "Prompting", "Inference Acceleration", "NLP", "Machine Learning", "Language Model"]

[tool.poetry.dependencies]
python = ">=3.9"
fschat = "^0.2.36"
torch = "^2.0.1"
transformers = "4.37.2"
accelerate = "^0.27.2"
peft = "^0.8.0"
datasets = ">=2.17.0"
numpy = ">=1.26.0"
bitsandbytes = "^0.42.0"
setuptools = "*"
sentencepiece = "*"
protobuf = "^4.25.3"
matplotlib = "*"
gradio = "*"


